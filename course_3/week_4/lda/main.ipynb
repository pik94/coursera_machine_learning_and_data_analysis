{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment: \n",
    "## Готовим LDA по рецептам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как вы уже знаете, в тематическом моделировании делается предположение о том, что для определения тематики порядок слов в документе не важен; об этом гласит гипотеза «мешка слов». Сегодня мы будем работать с несколько нестандартной для тематического моделирования коллекцией, которую можно назвать «мешком ингредиентов», потому что на состоит из рецептов блюд разных кухонь. Тематические модели ищут слова, которые часто вместе встречаются в документах, и составляют из них темы. Мы попробуем применить эту идею к рецептам и найти кулинарные «темы». Эта коллекция хороша тем, что не требует предобработки. Кроме того, эта задача достаточно наглядно иллюстрирует принцип работы тематических моделей.\n",
    "\n",
    "Для выполнения заданий, помимо часто используемых в курсе библиотек, потребуются модули *json* и *gensim*. Первый входит в дистрибутив Anaconda, второй можно поставить командой \n",
    "\n",
    "*pip install gensim*\n",
    "\n",
    "Построение модели занимает некоторое время. На ноутбуке с процессором Intel Core i7 и тактовой частотой 2400 МГц на построение одной модели уходит менее 10 минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллекция дана в json-формате: для каждого рецепта известны его id, кухня (cuisine) и список ингредиентов, в него входящих. Загрузить данные можно с помощью модуля json (он входит в дистрибутив Anaconda):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'greek',\n 'id': 10259,\n 'ingredients': ['romaine lettuce',\n  'black olives',\n  'grape tomatoes',\n  'garlic',\n  'pepper',\n  'purple onion',\n  'seasoning',\n  'garbanzo beans',\n  'feta cheese crumbles']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"recipes.json\") as f:\n",
    "    recipes = json.load(f)\n",
    "recipes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Составление корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша коллекция небольшая, и целиком помещается в оперативную память. Gensim может работать с такими данными и не требует их сохранения на диск в специальном формате. Для этого коллекция должна быть представлена в виде списка списков, каждый внутренний список соответствует отдельному документу и состоит из его слов. Пример коллекции из двух документов: \n",
    "\n",
    "[[\"hello\", \"world\"], [\"programming\", \"in\", \"python\"]]\n",
    "\n",
    "Преобразуем наши данные в такой формат, а затем создадим объекты corpus и dictionary, с которыми будет работать модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [recipe[\"ingredients\"] for recipe in recipes]\n",
    "dictionary = corpora.Dictionary(texts)   # составляем словарь\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]  # составляем корпус документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['romaine lettuce',\n 'black olives',\n 'grape tomatoes',\n 'garlic',\n 'pepper',\n 'purple onion',\n 'seasoning',\n 'garbanzo beans',\n 'feta cheese crumbles']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У объекта dictionary есть полезная переменная dictionary.token2id, позволяющая находить соответствие между ингредиентами и их индексами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели\n",
    "Вам может понадобиться [документация](https://radimrehurek.com/gensim/models/ldamodel.html) LDA в gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 1.__ Обучите модель LDA с 40 темами, установив количество проходов по коллекции 5 и оставив остальные параметры по умолчанию. \n",
    "\n",
    "\n",
    "Затем вызовите метод модели *show_topics*, указав количество тем 40 и количество токенов 10, и сохраните результат (топы ингредиентов в темах) в отдельную переменную. Если при вызове метода *show_topics* указать параметр *formatted=True*, то топы ингредиентов будет удобно выводить на печать, если *formatted=False*, будет удобно работать со списком программно. Выведите топы на печать, рассмотрите темы, а затем ответьте на вопрос:\n",
    "\n",
    "Сколько раз ингредиенты \"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\" встретились среди топов-10 всех 40 тем? При ответе __не нужно__ учитывать составные ингредиенты, например, \"hot water\".\n",
    "\n",
    "Передайте 6 чисел в функцию save_answers1 и загрузите сгенерированный файл в форму.\n",
    "\n",
    "У gensim нет возможности фиксировать случайное приближение через параметры метода, но библиотека использует numpy для инициализации матриц. Поэтому, по утверждению автора библиотеки, фиксировать случайное приближение нужно командой, которая написана в следующей ячейке. __Перед строкой кода с построением модели обязательно вставляйте указанную строку фиксации random.seed.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 45s, sys: 224 ms, total: 6min 45s\nWall time: 6min 45s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "%time model = models.ldamodel.LdaModel(corpus, num_topics=40, id2word=dictionary, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = model.show_topics(num_topics=40, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n  '0.084*\"garlic cloves\" + 0.068*\"chopped onion\" + 0.066*\"cooking spray\" + 0.063*\"salt\" + 0.043*\"water\" + 0.041*\"fat free less sodium chicken broth\" + 0.040*\"ground red pepper\" + 0.038*\"sliced green onions\" + 0.027*\"olive oil\" + 0.026*\"black pepper\"'),\n (1,\n  '0.082*\"corn kernels\" + 0.070*\"diced onions\" + 0.059*\"tortillas\" + 0.057*\"salt and ground black pepper\" + 0.047*\"1% low-fat milk\" + 0.040*\"sliced black olives\" + 0.039*\"cream cheese, soften\" + 0.038*\"vegetable stock\" + 0.038*\"chopped fresh chives\" + 0.035*\"lard\"'),\n (2,\n  '0.087*\"olive oil\" + 0.059*\"fresh basil\" + 0.049*\"salt\" + 0.047*\"fresh parsley\" + 0.046*\"crushed red pepper\" + 0.041*\"grated parmesan cheese\" + 0.041*\"garlic cloves\" + 0.030*\"cherry tomatoes\" + 0.027*\"butter\" + 0.027*\"low salt chicken broth\"'),\n (3,\n  '0.074*\"bacon\" + 0.072*\"salt\" + 0.061*\"red pepper flakes\" + 0.049*\"ground black pepper\" + 0.047*\"garlic\" + 0.042*\"pasta\" + 0.038*\"onions\" + 0.035*\"olive oil\" + 0.035*\"chicken thighs\" + 0.030*\"fresh dill\"'),\n (4,\n  '0.129*\"all-purpose flour\" + 0.116*\"large eggs\" + 0.089*\"salt\" + 0.080*\"milk\" + 0.076*\"butter\" + 0.050*\"sugar\" + 0.039*\"baking powder\" + 0.031*\"powdered sugar\" + 0.026*\"vegetable oil\" + 0.024*\"large egg whites\"'),\n (5,\n  '0.079*\"dry white wine\" + 0.049*\"olive oil\" + 0.044*\"shallots\" + 0.038*\"salt\" + 0.031*\"ground cloves\" + 0.030*\"ground allspice\" + 0.030*\"white wine vinegar\" + 0.029*\"butter\" + 0.029*\"arborio rice\" + 0.025*\"saffron threads\"'),\n (6,\n  '0.086*\"leeks\" + 0.072*\"dry red wine\" + 0.049*\"pork tenderloin\" + 0.044*\"reduced sodium soy sauce\" + 0.043*\"cilantro sprigs\" + 0.036*\"carrots\" + 0.034*\"peppercorns\" + 0.026*\"beef broth\" + 0.025*\"granny smith apples\" + 0.024*\"cremini mushrooms\"'),\n (7,\n  '0.088*\"soy sauce\" + 0.055*\"sesame oil\" + 0.046*\"scallions\" + 0.044*\"green onions\" + 0.039*\"corn starch\" + 0.039*\"rice vinegar\" + 0.036*\"sugar\" + 0.033*\"garlic\" + 0.031*\"vegetable oil\" + 0.027*\"fresh ginger\"'),\n (8,\n  '0.179*\"garlic powder\" + 0.106*\"cayenne pepper\" + 0.067*\"onion powder\" + 0.045*\"smoked paprika\" + 0.044*\"ground black pepper\" + 0.034*\"black pepper\" + 0.032*\"fine sea salt\" + 0.027*\"greek yogurt\" + 0.024*\"salt\" + 0.023*\"chili powder\"'),\n (9,\n  '0.063*\"olive oil\" + 0.062*\"purple onion\" + 0.061*\"salt\" + 0.060*\"fresh lemon juice\" + 0.053*\"garlic cloves\" + 0.052*\"extra-virgin olive oil\" + 0.047*\"plum tomatoes\" + 0.041*\"ground black pepper\" + 0.034*\"balsamic vinegar\" + 0.027*\"capers\"'),\n (10,\n  '0.053*\"broccoli florets\" + 0.050*\"lettuce leaves\" + 0.041*\"chili flakes\" + 0.040*\"crème fraîche\" + 0.037*\"radishes\" + 0.032*\"greek style plain yogurt\" + 0.031*\"pork sausages\" + 0.031*\"sharp cheddar cheese\" + 0.029*\"watercress\" + 0.027*\"quickcooking grits\"'),\n (11,\n  '0.101*\"lime juice\" + 0.090*\"lime\" + 0.051*\"salt\" + 0.046*\"garlic\" + 0.039*\"chopped cilantro\" + 0.038*\"purple onion\" + 0.035*\"fresh cilantro\" + 0.031*\"jalapeno chilies\" + 0.026*\"mango\" + 0.024*\"olive oil\"'),\n (12,\n  '0.082*\"cheese\" + 0.079*\"ricotta cheese\" + 0.075*\"orange juice\" + 0.050*\"sliced mushrooms\" + 0.050*\"baby spinach\" + 0.044*\"vegetable oil cooking spray\" + 0.032*\"frozen chopped spinach\" + 0.029*\"part-skim mozzarella cheese\" + 0.028*\"italian sausage\" + 0.027*\"part-skim ricotta cheese\"'),\n (13,\n  '0.077*\"diced tomatoes\" + 0.072*\"dried oregano\" + 0.056*\"onions\" + 0.051*\"tomato sauce\" + 0.049*\"salt\" + 0.044*\"garlic\" + 0.039*\"dried basil\" + 0.039*\"olive oil\" + 0.032*\"crushed tomatoes\" + 0.029*\"ground beef\"'),\n (14,\n  '0.067*\"beef\" + 0.048*\"onions\" + 0.047*\"green cabbage\" + 0.045*\"red wine\" + 0.042*\"green peas\" + 0.038*\"goat cheese\" + 0.033*\"beef stock\" + 0.031*\"red cabbage\" + 0.028*\"chili pepper\" + 0.025*\"paneer\"'),\n (15,\n  '0.143*\"lemon\" + 0.063*\"sugar\" + 0.060*\"fresh mint\" + 0.048*\"orange\" + 0.044*\"boiling water\" + 0.033*\"water\" + 0.033*\"cold water\" + 0.028*\"ground white pepper\" + 0.028*\"almonds\" + 0.023*\"all purpose unbleached flour\"'),\n (16,\n  '0.083*\"chopped cilantro fresh\" + 0.072*\"jalapeno chilies\" + 0.072*\"fresh lime juice\" + 0.055*\"white onion\" + 0.043*\"avocado\" + 0.040*\"salt\" + 0.037*\"ground cumin\" + 0.035*\"garlic cloves\" + 0.028*\"cilantro leaves\" + 0.026*\"tomatoes\"'),\n (17,\n  '0.122*\"zucchini\" + 0.067*\"eggplant\" + 0.064*\"onions\" + 0.052*\"olive oil\" + 0.051*\"garlic\" + 0.047*\"white wine\" + 0.042*\"chopped parsley\" + 0.037*\"fresh mushrooms\" + 0.035*\"lean ground beef\" + 0.035*\"raisins\"'),\n (18,\n  '0.107*\"parmesan cheese\" + 0.076*\"salt\" + 0.064*\"warm water\" + 0.052*\"bell pepper\" + 0.051*\"olive oil\" + 0.049*\"water\" + 0.036*\"grits\" + 0.031*\"polenta\" + 0.030*\"kale\" + 0.029*\"plain flour\"'),\n (19,\n  '0.135*\"unsalted butter\" + 0.072*\"all-purpose flour\" + 0.066*\"salt\" + 0.057*\"large eggs\" + 0.056*\"sugar\" + 0.042*\"granulated sugar\" + 0.042*\"whole milk\" + 0.032*\"buttermilk\" + 0.030*\"large egg yolks\" + 0.029*\"baking powder\"'),\n (20,\n  '0.087*\"red wine vinegar\" + 0.073*\"flat leaf parsley\" + 0.051*\"olive oil\" + 0.046*\"extra-virgin olive oil\" + 0.039*\"garlic cloves\" + 0.037*\"sweet onion\" + 0.036*\"ground black pepper\" + 0.036*\"tomatoes\" + 0.034*\"dry bread crumbs\" + 0.032*\"grated lemon zest\"'),\n (21,\n  '0.208*\"chicken broth\" + 0.085*\"boneless skinless chicken breast halves\" + 0.077*\"boneless skinless chicken breasts\" + 0.056*\"cajun seasoning\" + 0.037*\"juice\" + 0.033*\"butter\" + 0.028*\"pepper\" + 0.023*\"onions\" + 0.022*\"garlic salt\" + 0.021*\"boneless chicken breast\"'),\n (22,\n  '0.129*\"grated parmesan cheese\" + 0.050*\"mozzarella cheese\" + 0.046*\"shredded mozzarella cheese\" + 0.046*\"olive oil\" + 0.037*\"italian seasoning\" + 0.037*\"salt\" + 0.036*\"eggs\" + 0.034*\"garlic\" + 0.029*\"pepper\" + 0.027*\"pasta sauce\"'),\n (23,\n  '0.070*\"brown sugar\" + 0.048*\"soy sauce\" + 0.045*\"water\" + 0.044*\"salt\" + 0.043*\"white pepper\" + 0.039*\"oil\" + 0.038*\"sugar\" + 0.037*\"garlic\" + 0.036*\"sauce\" + 0.033*\"cooking oil\"'),\n (24,\n  '0.065*\"ground cumin\" + 0.045*\"salt\" + 0.041*\"ground coriander\" + 0.034*\"curry powder\" + 0.032*\"onions\" + 0.029*\"garlic\" + 0.024*\"tumeric\" + 0.024*\"garlic cloves\" + 0.024*\"fresh ginger\" + 0.023*\"vegetable oil\"'),\n (25,\n  '0.079*\"large shrimp\" + 0.070*\"hot water\" + 0.065*\"chopped garlic\" + 0.053*\"peanut oil\" + 0.046*\"rice wine\" + 0.036*\"hot red pepper flakes\" + 0.029*\"corn oil\" + 0.025*\"fontina cheese\" + 0.024*\"marsala wine\" + 0.024*\"seasoning\"'),\n (26,\n  '0.073*\"mirin\" + 0.064*\"chickpeas\" + 0.044*\"red pepper\" + 0.044*\"mint leaves\" + 0.044*\"spring onions\" + 0.042*\"fresh coriander\" + 0.037*\"sake\" + 0.035*\"saffron\" + 0.032*\"sugar\" + 0.030*\"soy sauce\"'),\n (27,\n  '0.168*\"heavy cream\" + 0.065*\"frozen peas\" + 0.057*\"grated nutmeg\" + 0.041*\"bread crumbs\" + 0.036*\"unsalted butter\" + 0.035*\"Tabasco Pepper Sauce\" + 0.034*\"bananas\" + 0.031*\"bread\" + 0.028*\"mascarpone\" + 0.027*\"lump crab meat\"'),\n (28,\n  '0.104*\"oil\" + 0.086*\"salt\" + 0.048*\"green chilies\" + 0.047*\"onions\" + 0.046*\"cilantro leaves\" + 0.045*\"cumin seed\" + 0.039*\"ground turmeric\" + 0.037*\"water\" + 0.030*\"chili powder\" + 0.030*\"red chili peppers\"'),\n (29,\n  '0.075*\"sour cream\" + 0.046*\"chili powder\" + 0.046*\"salsa\" + 0.045*\"flour tortillas\" + 0.040*\"shredded cheddar cheese\" + 0.039*\"black beans\" + 0.038*\"corn tortillas\" + 0.029*\"cilantro\" + 0.029*\"ground cumin\" + 0.027*\"onions\"'),\n (30,\n  '0.094*\"sugar\" + 0.080*\"whipping cream\" + 0.076*\"egg yolks\" + 0.055*\"vanilla extract\" + 0.044*\"butter\" + 0.042*\"half & half\" + 0.039*\"egg whites\" + 0.037*\"sweetened condensed milk\" + 0.032*\"water\" + 0.031*\"strawberries\"'),\n (31,\n  '0.099*\"fish sauce\" + 0.054*\"shallots\" + 0.044*\"coconut milk\" + 0.037*\"sugar\" + 0.033*\"vegetable oil\" + 0.031*\"fresh lime juice\" + 0.030*\"lemongrass\" + 0.028*\"water\" + 0.023*\"medium shrimp\" + 0.023*\"beansprouts\"'),\n (32,\n  '0.133*\"eggs\" + 0.091*\"salt\" + 0.059*\"butter\" + 0.056*\"all-purpose flour\" + 0.055*\"milk\" + 0.053*\"flour\" + 0.052*\"white sugar\" + 0.036*\"sugar\" + 0.032*\"baking powder\" + 0.021*\"water\"'),\n (33,\n  '0.072*\"rice\" + 0.071*\"chicken breasts\" + 0.047*\"coriander\" + 0.045*\"cabbage\" + 0.040*\"salt\" + 0.039*\"onions\" + 0.039*\"ghee\" + 0.029*\"garlic\" + 0.029*\"broccoli\" + 0.027*\"ground cardamom\"'),\n (34,\n  '0.077*\"cinnamon sticks\" + 0.072*\"clove\" + 0.049*\"black peppercorns\" + 0.046*\"garam masala\" + 0.035*\"chopped tomatoes\" + 0.035*\"cream\" + 0.033*\"garlic paste\" + 0.032*\"onions\" + 0.029*\"yoghurt\" + 0.028*\"coriander seeds\"'),\n (35,\n  '0.060*\"onions\" + 0.053*\"salt\" + 0.049*\"green bell pepper\" + 0.044*\"celery\" + 0.031*\"water\" + 0.030*\"dried thyme\" + 0.029*\"shrimp\" + 0.028*\"garlic\" + 0.028*\"bay leaves\" + 0.025*\"hot sauce\"'),\n (36,\n  '0.097*\"salt\" + 0.078*\"paprika\" + 0.065*\"onions\" + 0.064*\"pepper\" + 0.062*\"potatoes\" + 0.031*\"carrots\" + 0.030*\"butter\" + 0.029*\"worcestershire sauce\" + 0.028*\"garlic\" + 0.027*\"fresh thyme\"'),\n (37,\n  '0.124*\"extra-virgin olive oil\" + 0.075*\"freshly ground pepper\" + 0.066*\"sea salt\" + 0.049*\"large garlic cloves\" + 0.043*\"coarse salt\" + 0.040*\"kosher salt\" + 0.039*\"garlic cloves\" + 0.029*\"celery ribs\" + 0.029*\"ground pepper\" + 0.028*\"parmigiano reggiano cheese\"'),\n (38,\n  '0.080*\"chicken\" + 0.078*\"chicken stock\" + 0.055*\"white vinegar\" + 0.054*\"yellow onion\" + 0.047*\"onions\" + 0.045*\"garlic\" + 0.042*\"ground black pepper\" + 0.033*\"kosher salt\" + 0.028*\"water\" + 0.027*\"salt\"'),\n (39,\n  '0.127*\"mayonaise\" + 0.065*\"cheddar cheese\" + 0.061*\"dijon mustard\" + 0.057*\"cider vinegar\" + 0.057*\"cracked black pepper\" + 0.039*\"roma tomatoes\" + 0.038*\"apple cider vinegar\" + 0.036*\"white rice\" + 0.036*\"lemon wedge\" + 0.023*\"french bread\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.show_topics(num_topics=40, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chicken': 1, 'eggs': 2, 'mushrooms': 0, 'salt': 21, 'sugar': 9, 'water': 10}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingredients = [\"salt\", \"sugar\", \"water\", \"mushrooms\", \"chicken\", \"eggs\"]\n",
    "\n",
    "result = dict(zip(ingredients, [0] * 6))\n",
    "\n",
    "for ingr in ingredients:\n",
    "    for topic in topics:\n",
    "        for top_word in topic[1]:\n",
    "            if top_word[0] == ingr:\n",
    "                result[ingr] += 1\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 9, 10, 0, 1, 2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [result.get(ingr) for ingr in ingredients]\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_answers1(c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs):\n",
    "    with open(\"cooking_LDA_pa_task1.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [c_salt, c_sugar, c_water, c_mushrooms, c_chicken, c_eggs]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers1(values[0], values[1], values[2], values[3], values[4], values[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фильтрация словаря\n",
    "В топах тем гораздо чаще встречаются первые три рассмотренных ингредиента, чем последние три. При этом наличие в рецепте курицы, яиц и грибов яснее дает понять, что мы будем готовить, чем наличие соли, сахара и воды. Таким образом, даже в рецептах есть слова, часто встречающиеся в текстах и не несущие смысловой нагрузки, и поэтому их не желательно видеть в темах. Наиболее простой прием борьбы с такими фоновыми элементами — фильтрация словаря по частоте. Обычно словарь фильтруют с двух сторон: убирают очень редкие слова (в целях экономии памяти) и очень частые слова (в целях повышения интерпретируемости тем). Мы уберем только частые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "dictionary_2 = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 2.__ У объекта dictionary2 есть переменная *dfs* — это словарь, ключами которого являются id токена, а элементами — число раз, сколько слово встретилось во всей коллекции. Сохраните в отдельный список ингредиенты, которые встретились в коллекции больше 4000 раз. Вызовите метод словаря *filter_tokens*, подав в качестве первого аргумента полученный список популярных ингредиентов. Вычислите две величины: dict_size_before и dict_size_after — размер словаря до и после фильтрации.\n",
    "\n",
    "Затем, используя новый словарь, создайте новый корпус документов, corpus2, по аналогии с тем, как это сделано в начале ноутбука. Вычислите две величины: corpus_size_before и corpus_size_after — суммарное количество ингредиентов в корпусе (для каждого документа вычислите число различных ингредиентов в нем и просуммируйте по всем документам) до и после фильтрации.\n",
    "\n",
    "Передайте величины dict_size_before, dict_size_after, corpus_size_before, corpus_size_after в функцию save_answers2 и загрузите сгенерированный файл в форму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_size_before = len(dictionary.dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 11, 15, 18, 20, 29, 44, 52, 59, 104, 114]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_ind_words = [key for key in dictionary_2.dfs.keys() if dictionary_2.dfs[key] > 4000]\n",
    "extra_ind_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6714\n6702\n"
     ]
    }
   ],
   "source": [
    "dictionary_2.filter_tokens(extra_ind_words)\n",
    "dict_size_after = len(dictionary_2.dfs)\n",
    "print(dict_size_before)\n",
    "print(dict_size_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428249\n343665\n"
     ]
    }
   ],
   "source": [
    "corpus_2 = [dictionary_2.doc2bow(text) for text in texts]\n",
    "corpus_size_before = 0\n",
    "corpus_size_after = 0\n",
    "\n",
    "for c in corpus:\n",
    "    corpus_size_before += len(c)\n",
    "\n",
    "for c in corpus_2:\n",
    "    corpus_size_after += len(c)\n",
    "    \n",
    "print(corpus_size_before)\n",
    "print(corpus_size_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after):\n",
    "    with open(\"cooking_LDA_pa_task2.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [dict_size_before, dict_size_after, corpus_size_before, corpus_size_after]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers2(dict_size_before, dict_size_after, corpus_size_before, corpus_size_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение когерентностей\n",
    "__Задание 3.__ Постройте еще одну модель по корпусу corpus2 и словарю dictionary2, остальные параметры оставьте такими же, как при первом построении модели. Сохраните новую модель в другую переменную (не перезаписывайте предыдущую модель). Не забудьте про фиксирование seed!\n",
    "\n",
    "Затем воспользуйтесь методом *top_topics* модели, чтобы вычислить ее когерентность. Передайте в качестве аргумента соответствующий модели корпус. Метод вернет список кортежей (топ токенов, когерентность), отсортированных по убыванию последней. Вычислите среднюю по всем темам когерентность для каждой из двух моделей и передайте в функцию save_answers3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 50s, sys: 88 ms, total: 5min 50s\nWall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "%time second_model = models.ldamodel.LdaModel(corpus_2, num_topics=40, id2word=dictionary_2, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(0.07470618, 'sour cream'), (0.04646297, 'chili powder'), (0.045566555, 'salsa'), (0.04498307, 'flour tortillas'), (0.03975233, 'shredded cheddar cheese'), (0.039275248, 'black beans'), (0.037687555, 'corn tortillas'), (0.02913432, 'cilantro'), (0.028640373, 'ground cumin'), (0.027310649, 'onions'), (0.026456006, 'salt'), (0.02404393, 'green onions'), (0.02113124, 'avocado'), (0.020965634, 'cumin'), (0.01812089, 'tomatoes'), (0.017340321, 'jalapeno chilies'), (0.01663795, 'olive oil'), (0.0152053675, 'corn'), (0.015093604, 'enchilada sauce'), (0.014856199, 'garlic')], -2.189890114250082), ([(0.08774398, 'soy sauce'), (0.054586165, 'sesame oil'), (0.0461808, 'scallions'), (0.044415973, 'green onions'), (0.03923337, 'corn starch'), (0.03877022, 'rice vinegar'), (0.036451384, 'sugar'), (0.032991447, 'garlic'), (0.031374607, 'vegetable oil'), (0.02654085, 'fresh ginger'), (0.025166744, 'water'), (0.025108837, 'ginger'), (0.022585064, 'sesame seeds'), (0.020602163, 'salt'), (0.020497594, 'carrots'), (0.016626628, 'garlic cloves'), (0.013841002, 'honey'), (0.013411739, 'minced garlic'), (0.012145321, 'toasted sesame oil'), (0.01135009, 'canola oil')], -2.3812265876644365), ([(0.06536905, 'ground cumin'), (0.04451146, 'salt'), (0.041455127, 'ground coriander'), (0.03357062, 'curry powder'), (0.032030355, 'onions'), (0.029490048, 'garlic'), (0.024187999, 'tumeric'), (0.023919014, 'garlic cloves'), (0.02350857, 'fresh ginger'), (0.02328796, 'vegetable oil'), (0.022693198, 'garam masala'), (0.022626564, 'ground turmeric'), (0.020257086, 'cayenne'), (0.019907946, 'paprika'), (0.019101068, 'chopped cilantro fresh'), (0.018704215, 'olive oil'), (0.01809321, 'water'), (0.017506039, 'ground cinnamon'), (0.017384509, 'cayenne pepper'), (0.015291513, 'kosher salt')], -2.4231013694600465)]\n[([(0.09989068, 'sour cream'), (0.060923547, 'salsa'), (0.06014342, 'flour tortillas'), (0.04832878, 'chili powder'), (0.046898056, 'black beans'), (0.0376288, 'ground cumin'), (0.036287073, 'corn tortillas'), (0.027721073, 'cheese'), (0.027575668, 'cheddar cheese'), (0.026183944, 'green onions'), (0.02602243, 'shredded cheddar cheese'), (0.022421239, 'shredded Monterey Jack cheese'), (0.0201805, 'enchilada sauce'), (0.020101244, 'cooked chicken'), (0.017868262, 'cumin'), (0.017837185, 'green chile'), (0.01783637, 'diced tomatoes'), (0.015898595, 'tomatoes'), (0.015708098, 'chicken broth'), (0.015495267, 'chopped cilantro fresh')], -2.6820476724476943), ([(0.08161488, 'jalapeno chilies'), (0.060154695, 'lime'), (0.05236055, 'chopped cilantro fresh'), (0.05198337, 'avocado'), (0.046289302, 'lime juice'), (0.046241686, 'purple onion'), (0.043073602, 'fresh lime juice'), (0.037746098, 'cilantro'), (0.034761623, 'ground cumin'), (0.034696203, 'fresh cilantro'), (0.03434915, 'chopped cilantro'), (0.030662121, 'tomatoes'), (0.02631327, 'kosher salt'), (0.023741275, 'cilantro leaves'), (0.023202315, 'corn tortillas'), (0.02054616, 'white onion'), (0.018276086, 'tomatillos'), (0.01780063, 'lime wedges'), (0.014871639, 'serrano chile'), (0.01460175, 'chili powder')], -2.7313580184867887), ([(0.10913136, 'soy sauce'), (0.05570592, 'sesame oil'), (0.04614046, 'green onions'), (0.044368435, 'scallions'), (0.044250157, 'corn starch'), (0.040481403, 'rice vinegar'), (0.03208615, 'ginger'), (0.023920765, 'fresh ginger'), (0.023214702, 'carrots'), (0.019680636, 'sesame seeds'), (0.01634862, 'brown sugar'), (0.01587118, 'oil'), (0.015269662, 'peanut oil'), (0.014883747, 'mirin'), (0.014075671, 'oyster sauce'), (0.014037563, 'honey'), (0.014001641, 'minced garlic'), (0.013170014, 'hoisin sauce'), (0.012618228, 'eggs'), (0.01193205, 'ground pork')], -2.7682497157167134)]\n"
     ]
    }
   ],
   "source": [
    "coh  = model.top_topics(corpus)\n",
    "coh_2 = second_model.top_topics(corpus_2)\n",
    "print(coh[:3])\n",
    "print(coh_2[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.189890114250082, -2.3812265876644365, -2.4231013694600465, -2.6404301145967835, -2.7763520807543136, -2.8454041504995753, -3.000492762534722, -3.018488360769831, -3.132109412565028, -3.145299180624706, -3.157187161721497, -3.2629401970283376, -3.2801800797009295, -3.3570320376187595, -3.386796198092065, -3.4862641247234962, -3.737235228840055, -3.7815922439619656, -3.820541568356231, -4.019589387903069, -4.075628937421615, -4.436982949171743, -4.608996583865164, -4.8115250226059585, -4.838593993196258, -4.854696204845901, -5.875580559035723, -6.072235653328083, -6.199675161075947, -7.795644693624812, -8.668198601897082, -9.387545119830756, -10.961139042754104, -11.089534095841602, -11.126384474736245, -11.497323165418242, -13.22178490430017, -14.652786676785686, -16.50949443870023, -17.297955995958]\n[-2.6820476724476943, -2.7313580184867887, -2.7682497157167134, -2.918380327259511, -3.1533236398379736, -3.188420639293828, -3.7820034230857225, -3.8278936211965666, -3.860168161324259, -4.097754592144618, -4.40186206775327, -4.428750238755899, -5.544630148205429, -5.683974387071971, -5.958438669138916, -6.577139760624397, -6.6406563929324065, -6.86890826354955, -7.034502843311678, -7.050893289259838, -7.189833778018548, -7.2727103444784955, -8.278954925155327, -8.469686600961758, -8.79483935090436, -9.204505732709553, -9.416265678447973, -10.16678432179765, -10.56378045992699, -11.8285951421701, -12.032810984465184, -12.198231164851318, -12.37019979969584, -13.807214410180885, -13.94474566347716, -14.042003385744637, -15.276951337868073, -15.897524398862041, -17.844271105595663, -18.041888655614446]\n"
     ]
    }
   ],
   "source": [
    "nums = [c[1] for c in coh]\n",
    "nums_2 = [c[1] for c in coh_2]\n",
    "print(nums)\n",
    "print(nums_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_answers3(coherence, coherence2):\n",
    "    with open(\"cooking_LDA_pa_task3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([\"%3f\"%el for el in [coherence, coherence2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers3(np.mean(nums), np.mean(nums_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считается, что когерентность хорошо соотносится с человеческими оценками интерпретируемости тем. Поэтому на больших текстовых коллекциях когерентность обычно повышается, если убрать фоновую лексику. Однако в нашем случае этого не произошло. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изучение влияния гиперпараметра alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе мы будем работать со второй моделью, то есть той, которая построена по сокращенному корпусу. \n",
    "\n",
    "Пока что мы посмотрели только на матрицу темы-слова, теперь давайте посмотрим на матрицу темы-документы. Выведите темы для нулевого (или любого другого) документа из корпуса, воспользовавшись методом *get_document_topics* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, 0.128125), (30, 0.13281512), (31, 0.6234349)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model.get_document_topics(corpus_2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также выведите содержимое переменной *.alpha* второй модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n       0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n       0.025, 0.025, 0.025, 0.025], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У вас должно получиться, что документ характеризуется небольшим числом тем. Попробуем поменять гиперпараметр alpha, задающий априорное распределение Дирихле для распределений тем в документах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задание 4.__ Обучите третью модель: используйте сокращенный корпус (corpus2 и dictionary2) и установите параметр __alpha=1__, passes=5. Не забудьте про фиксацию seed! Выведите темы новой модели для нулевого документа; должно получиться, что распределение над множеством тем практически равномерное. Чтобы убедиться в том, что во второй модели документы описываются гораздо более разреженными распределениями, чем в третьей, посчитайте суммарное количество элементов, __превосходящих 0.01__, в матрицах темы-документы обеих моделей. Другими словами, запросите темы  модели для каждого документа с параметром *minimum_probability=0.01* и просуммируйте число элементов в получаемых массивах. Передайте две суммы (сначала для модели с alpha по умолчанию, затем для модели в alpha=1) в функцию save_answers4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 9s, sys: 600 ms, total: 18min 9s\nWall time: 18min 10s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(76543)\n",
    "# здесь код для построения модели:\n",
    "%time third_model = models.ldamodel.LdaModel(corpus_2, num_topics=40, id2word=dictionary_2, passes=5, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.021348575),\n (1, 0.021298638),\n (2, 0.021276595),\n (3, 0.021338549),\n (4, 0.02128087),\n (5, 0.021382393),\n (6, 0.021299792),\n (7, 0.021277202),\n (8, 0.021449137),\n (9, 0.021319209),\n (10, 0.02164358),\n (11, 0.042108793),\n (12, 0.021276595),\n (13, 0.046975877),\n (14, 0.021555642),\n (15, 0.021562053),\n (16, 0.021459104),\n (17, 0.021820657),\n (18, 0.021276603),\n (19, 0.021417111),\n (20, 0.024527254),\n (21, 0.021288445),\n (22, 0.021276595),\n (23, 0.021283576),\n (24, 0.021810684),\n (25, 0.021577844),\n (26, 0.021523274),\n (27, 0.02134925),\n (28, 0.021500375),\n (29, 0.02132352),\n (30, 0.042586245),\n (31, 0.0710489),\n (32, 0.021418689),\n (33, 0.021310415),\n (34, 0.02129087),\n (35, 0.021343203),\n (36, 0.021288779),\n (37, 0.021294925),\n (38, 0.021279102),\n (39, 0.045311056)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_model.get_document_topics(corpus_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_answers4(count_model2, count_model3):\n",
    "    with open(\"cooking_LDA_pa_task4.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(el) for el in [count_model2, count_model3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_1 = second_model.get_document_topics(corpus_2, minimum_probability=0.01)\n",
    "distr_2 = third_model.get_document_topics(corpus_2, minimum_probability=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_1 = 0\n",
    "for x in distr_1:\n",
    "    size_1 += len(x)\n",
    "    \n",
    "size_2 = 0\n",
    "for x in distr_2:\n",
    "    size_2 += len(x)\n",
    "    \n",
    "save_answers4(size_1, size_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, гиперпараметр __alpha__ влияет на разреженность распределений тем в документах. Аналогично гиперпараметр __eta__ влияет на разреженность распределений слов в темах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA как способ понижения размерности\n",
    "Иногда, распределения над темами, найденные с помощью LDA, добавляют в матрицу объекты-признаки как дополнительные, семантические, признаки, и это может улучшить качество решения задачи. Для простоты давайте просто обучим классификатор рецептов на кухни на признаках, полученных из LDA, и измерим точность (accuracy).\n",
    "\n",
    "__Задание 5.__ Используйте модель, построенную по сокращенной выборке с alpha по умолчанию (вторую модель). Составьте матрицу $\\Theta = p(t|d)$ вероятностей тем в документах; вы можете использовать тот же метод get_document_topics, а также вектор правильных ответов y (в том же порядке, в котором рецепты идут в переменной recipes). Создайте объект RandomForestClassifier со 100 деревьями, с помощью функции cross_val_score вычислите среднюю accuracy по трем фолдам (перемешивать данные не нужно) и передайте в функцию save_answers5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pik/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0025),\n (1, 0.0025),\n (2, 0.0025),\n (3, 0.0025),\n (4, 0.0025),\n (5, 0.0025),\n (6, 0.0025),\n (7, 0.0025),\n (8, 0.0025),\n (9, 0.0025),\n (10, 0.0025),\n (11, 0.10438007),\n (12, 0.0025),\n (13, 0.0025),\n (14, 0.0025),\n (15, 0.1025),\n (16, 0.0025),\n (17, 0.12853052),\n (18, 0.0025),\n (19, 0.0025),\n (20, 0.0025),\n (21, 0.1025),\n (22, 0.16307384),\n (23, 0.21120591),\n (24, 0.0025),\n (25, 0.0025),\n (26, 0.0025),\n (27, 0.0025),\n (28, 0.0025),\n (29, 0.10530964),\n (30, 0.0025),\n (31, 0.0025),\n (32, 0.0025),\n (33, 0.0025),\n (34, 0.0025),\n (35, 0.0025),\n (36, 0.0025),\n (37, 0.0025),\n (38, 0.0025),\n (39, 0.0025)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = second_model.get_document_topics(corpus_2, minimum_probability=0)\n",
    "topics[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.zeros([len(topics), 40])\n",
    "for i, topic in enumerate(topics):\n",
    "    for t in topic:\n",
    "        p[i, t[0]] = t[1]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['greek', 'southern_us', 'filipino', 'indian', 'indian']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [recipe[\"cuisine\"] for recipe in recipes]\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_answers5(accuracy):\n",
    "     with open(\"cooking_LDA_pa_task5.txt\", \"w\") as fout:\n",
    "        fout.write(str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55266531, 0.54468663, 0.55999094])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(RandomForestClassifier(n_estimators=100), p, y)\n",
    "score[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_answers5(score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для такого большого количества классов это неплохая точность. Вы можете попроовать обучать RandomForest на исходной матрице частот слов, имеющей значительно большую размерность, и увидеть, что accuracy увеличивается на 10–15%. Таким образом, LDA собрал не всю, но достаточно большую часть информации из выборки, в матрице низкого ранга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA — вероятностная модель\n",
    "Матричное разложение, использующееся в LDA, интерпретируется как следующий процесс генерации документов.\n",
    "\n",
    "Для документа $d$ длины $n_d$:\n",
    "1. Из априорного распределения Дирихле с параметром alpha сгенерировать распределение над множеством тем: $\\theta_d \\sim Dirichlet(\\alpha)$\n",
    "1. Для каждого слова $w = 1, \\dots, n_d$:\n",
    "    1. Сгенерировать тему из дискретного распределения $t \\sim \\theta_{d}$\n",
    "    1. Сгенерировать слово из дискретного распределения $w \\sim \\phi_{t}$.\n",
    "    \n",
    "Подробнее об этом в [Википедии](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation).\n",
    "\n",
    "В контексте нашей задачи получается, что, используя данный генеративный процесс, можно создавать новые рецепты. Вы можете передать в функцию модель и число ингредиентов и сгенерировать рецепт :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(model, num_ingredients):\n",
    "    theta = np.random.dirichlet(model.alpha)\n",
    "    for i in range(num_ingredients):\n",
    "        t = np.random.choice(np.arange(model.num_topics), p=theta)\n",
    "        topic = model.show_topic(t, topn=model.num_terms)\n",
    "        topic_distr = [x[1] for x in topic]\n",
    "        terms = [x[0] for x in topic]\n",
    "        w = np.random.choice(terms, p=topic_distr)\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_recipe(model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация построенной модели\n",
    "Вы можете рассмотреть топы ингредиентов каждой темы. Большиснтво тем сами по себе похожи на рецепты; в некоторых собираются продукты одного вида, например, свежие фрукты или разные виды сыра.\n",
    "\n",
    "Попробуем эмпирически соотнести наши темы с национальными кухнями (cuisine). Построим матрицу $A$ размера темы $x$ кухни, ее элементы $a_{tc}$ — суммы $p(t|d)$ по всем документам $d$, которые отнесены к кухне $c$. Нормируем матрицу на частоты рецептов по разным кухням, чтобы избежать дисбаланса между кухнями. Следующая функция получает на вход объект модели, объект корпуса и исходные данные и возвращает нормированную матрицу $A$. Ее удобно визуализировать с помощью seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import seaborn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_topic_cuisine_matrix(model, corpus, recipes):\n",
    "    # составляем вектор целевых признаков\n",
    "    targets = list(set([recipe[\"cuisine\"] for recipe in recipes]))\n",
    "    # составляем матрицу\n",
    "    tc_matrix = pandas.DataFrame(data=np.zeros((model.num_topics, len(targets))), columns=targets)\n",
    "    for recipe, bow in zip(recipes, corpus):\n",
    "        recipe_topic = model.get_document_topics(bow)\n",
    "        for t, prob in recipe_topic:\n",
    "            tc_matrix[recipe[\"cuisine\"]][t] += prob\n",
    "    # нормируем матрицу\n",
    "    target_sums = pandas.DataFrame(data=np.zeros((1, len(targets))), columns=targets)\n",
    "    for recipe in recipes:\n",
    "        target_sums[recipe[\"cuisine\"]] += 1\n",
    "    return pandas.DataFrame(tc_matrix.values/target_sums.values, columns=tc_matrix.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_matrix(tc_matrix):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    seaborn.heatmap(tc_matrix, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Визуализируйте матрицу\n",
    "matrix = compute_topic_cuisine_matrix(model, corpus, recipes)\n",
    "plot_matrix(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем темнее квадрат в матрице, тем больше связь этой темы с данной кухней. Мы видим, что у нас есть темы, которые связаны с несколькими кухнями. Такие темы показывают набор ингредиентов, которые популярны в кухнях нескольких народов, то есть указывают на схожесть кухонь этих народов. Некоторые темы распределены по всем кухням равномерно, они показывают наборы продуктов, которые часто используются в кулинарии всех стран. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жаль, что в датасете нет названий рецептов, иначе темы было бы проще интерпретировать..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение\n",
    "В этом задании вы построили несколько моделей LDA, посмотрели, на что влияют гиперпараметры модели и как можно использовать построенную модель. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
